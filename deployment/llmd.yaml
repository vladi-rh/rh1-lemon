---
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: openshift-ai-inference
spec:
  controllerName: openshift.io/gateway-controller/v1
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  labels:
    istio.io/rev: openshift-gateway
  name: openshift-ai-inference
  namespace: openshift-ingress
spec:
  gatewayClassName: openshift-ai-inference
  listeners:
    - allowedRoutes:
        namespaces:
          from: All
      name: http
      port: 80
      protocol: HTTP
---
apiVersion: v1
kind: Namespace
metadata:
  name: lemonade-stand
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  annotations:
    opendatahub.io/model-type: generative
    openshift.io/display-name: llama-llm-d
    security.opendatahub.io/enable-auth: 'false'
  name: llama-llm-d
  namespace: lemonade-stand
  finalizers:
    - serving.kserve.io/llmisvc-finalizer
spec:
  model:
    name: llama32
    uri: oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct
  replicas: 47 # change replicas as needed
  router:
    gateway: {}
    route: {}
    scheduler:
      template:
        containers:
          - name: main
            resources:
              limits:
                cpu: '8'
                memory: 24Gi
              requests:
                cpu: '6'
                memory: 16Gi
            args:
              - --pool-group
              - inference.networking.x-k8s.io
              - '--pool-name'
              - '{{ ChildName .ObjectMeta.Name `-inference-pool` }}'
              - '--pool-namespace'
              - '{{ .ObjectMeta.Namespace }}'
              - '--zap-encoder'
              - json
              - '--grpc-port'
              - '9002'
              - '--grpc-health-port'
              - '9003'
              - '--secure-serving'
              - '--model-server-metrics-scheme'
              - https
              - '--config-text'
              - |
                apiVersion: inference.networking.x-k8s.io/v1alpha1
                kind: EndpointPickerConfig
                plugins:
                - type: single-profile-handler
                - type: queue-scorer
                - type: kv-cache-utilization-scorer
                schedulingProfiles:
                - name: default
                  plugins:
                  - pluginRef: queue-scorer
                    weight: 3
                  - pluginRef: kv-cache-utilization-scorer
                    weight: 2
  template:
    containers:
      - env:
          - name: VLLM_ADDITIONAL_ARGS
            value: '--model=/mnt/models --served-model-name=llama32 --disable-uvicorn-access-log --disable-log-requests --max-model-len=512 --gpu-memory-utilization=0.90 --max-num-batched-tokens=8192 --max-num-seqs=160 --enable-chunked-prefill'
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 30
        name: main
        resources:
          limits:
            cpu: '2'
            memory: 24Gi
            nvidia.com/gpu: '1'
          requests:
            cpu: '1'
            memory: 16Gi
            nvidia.com/gpu: '1'
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
